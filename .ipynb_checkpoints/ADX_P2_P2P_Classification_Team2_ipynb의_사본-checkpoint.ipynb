{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCzqQv-lpXNK"
      },
      "source": [
        "# Project 2. P2P  대출심사 알고리듬\n",
        "**과목명:  응용데이터애널리틱스 (Applied Data Analytics)** \\\\\n",
        "**학수번호:  INE4108** \\\\\n",
        "**교강사:  송재욱 교수 (산업공학과)** \\\\\n",
        " \\\\\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21F0poqDnriY"
      },
      "source": [
        "## 1. Environmental Set-up & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16gjgF_8nmu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635b4399-2344-4b1b-cf8e-c4ea4e43b530"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Google Drive where you have saved the unzipped\n",
        "FOLDERNAME =  'ADX/'\n",
        "\n",
        "assert FOLDERNAME is not None, 'ERROR'\n",
        "\n",
        "%cd drive/My\\ Drive\n",
        "%cp -r $FOLDERNAME ../../\n",
        "\n",
        "# 모델 수립을 위한 Train/Validation Set\n",
        "df = pd.read_csv('/content/drive/MyDrive/ADX/P2_P2P/P2_dataset.csv', encoding ='cp949') \n",
        "\n",
        "# 시스템 구현을 위한 Test Set 샘플\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/ADX/P2_P2P/P2_dataset_test_sample.csv', encoding ='cp949')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "cp: cannot open 'ADX/B.P/dataset.gsheet' for reading: Operation not supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv3tA_28ovjx"
      },
      "source": [
        "## 2. Modelling\n",
        "- df를 사용하여 예측모형 수립\n",
        "- Feature engineering에 대한 토의/구현 진행\n",
        "- Computation time을 고려하여 전체 데이터를 완전히 사용하지 말고 Sampling하여 Model Train/Vadliation을 진행하는 것을 추천\n",
        "- Sampling을 단 한번 한 것으로 모형 Train 한 것이 과연 옳은 것인지에 대해 팀원들과 고민해볼 것. 해결 방법은 없을지 서치해보는 것도 하나의 task임."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xOQcwOv3Q9C"
      },
      "source": [
        "# import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.dropna(axis=0)\n",
        "df2 = df1[['total_pymnt','total_pymnt_inv','total_rec_late_fee','recoveries','collection_recovery_fee',\n",
        "          'last_pymnt_amnt','int_rate','verification_status','loan_status']]\n",
        "          \n",
        "df2 = df2.replace({'loan_status' :'Charged Off'},0)\n",
        "df2 = df2.replace({'loan_status' :'Fully Paid'},1)\n",
        "df2 = df2.replace({'initial_list_status':'w'},0)\n",
        "df2 = df2.replace({'initial_list_status':'f'},1)\n",
        "\n",
        "df2['verification_status'].replace('Not Verified', 0, inplace=True)\n",
        "df2['verification_status'].replace('Source Verified', 1, inplace=True)\n",
        "df2['verification_status'].replace('Verified', 1, inplace=True)"
      ],
      "metadata": {
        "id": "K6vFtyjWTcOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "loan_0 = df2[df2['loan_status']==0]\n",
        "loan_0 = loan_0.sample(n=int(loan_0.shape[0]*0.1), random_state=42)\n",
        "\n",
        "loan_1 = df2[df2['loan_status']==1]\n",
        "loan_1 = loan_1.sample(n=int(loan_1.shape[0]*0.1), random_state=42)\n",
        "\n",
        "dfs = pd.concat([loan_0,loan_1])\n",
        "\n",
        "print(dfs['loan_status'].value_counts())\n",
        "\n",
        "X_ = dfs.drop(['loan_status'], axis=1)\n",
        "y_ = dfs['loan_status']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.2)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NHjTZyvZPbA",
        "outputId": "af4802f8-2bb8-47ac-ceb7-bd0e09181f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    26415\n",
            "0     3858\n",
            "Name: loan_status, dtype: int64\n",
            "(24218, 8)\n",
            "(6055, 8)\n",
            "(24218,)\n",
            "(6055,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cvxopt import matrix as cvxopt_matrix\n",
        "from cvxopt import solvers as cvxopt_solvers\n",
        "\n",
        "def Kernel_(x, y, params = 0, type_ = 'default') :\n",
        "    if type_ == 'rbf' :\n",
        "      Kernel = np.exp(- (np.sum(x **2, axis = 1).reshape(-1,1) + np.sum(y **2, axis = 1).reshape(1,-1) - 2 * x @ y.T)* params)\n",
        "      return Kernel\n",
        "    elif type_ == 'default' :\n",
        "      Kernel = np.dot(x, y.T)\n",
        "      return Kernel\n",
        "\n",
        "def Minmax_(X) :\n",
        "    return (X - X.min(axis = 0)) / (X.max(axis= 0) - X.min(axis = 0)), X.max(axis =0) , X.min(axis =0)\n",
        "\n",
        "def Standar_(X) :\n",
        "    return (X - X.mean(axis =0)) / X.std(axis = 0), X.mean(axis =0), X.std(axis = 0)\n",
        "\n",
        "def Minmax_val(X, min, max) :\n",
        "  return (X - min) / (max - min)\n",
        "\n",
        "def Standar_val(X, avg, std) :\n",
        "  return (X - avg) / std\n",
        "\n",
        "def Convolution(pred, real) :\n",
        "    pred = np.array(pred)\n",
        "    y = np.array(real)\n",
        "    TP = np.sum((pred == 1) & (y == 1))\n",
        "    FP = np.sum((pred == 1) & (y != 1))\n",
        "    FN = np.sum((pred != 1) & (y == 1))\n",
        "    TN = np.sum((pred != 1) & (y != 1))\n",
        "    return TP, FP, FN, TN\n",
        "\n",
        "############## Accuracy is newly included!\n",
        "def acc_precision_recall(X) :\n",
        "  TP,FP,FN,TN = X\n",
        "  return (TP + TN) / (TP + FP + FN + TN), TP / (TP + FP), TP / (TP + FN)"
      ],
      "metadata": {
        "id": "ZQuIks0OUSLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing values and computing H matrix. Note the 1. to force to float type\n",
        "C = 10000  # 여기 C는 hyperparameter\n",
        "\n",
        "X, cache_avg, cache_std = Standar_(np.array(X_train))\n",
        "X = np.nan_to_num(X)\n",
        "\n",
        "y = np.array(y_train)* 1.\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "X_val = Standar_val(np.array(X_val), avg=cache_avg, std=cache_std)\n",
        "X_val = np.nan_to_num(X_val)\n",
        "\n",
        "y_val = np.array(y_val) * 1.\n",
        "y_val = y_val.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "f_aMbwzILLnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kernel 관련 정의 numpy array 형식으로 생성\n",
        "H_bench = Kernel_(X, X) * 1.\n",
        "H_bench *= y@y.T"
      ],
      "metadata": {
        "id": "6OpkuvNBfdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_bench = cvxopt_matrix(H_bench)"
      ],
      "metadata": {
        "id": "T3QKy8cWfjdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_bench = cvxopt_matrix(-np.ones((X.shape[0], 1)))"
      ],
      "metadata": {
        "id": "tO_UKETjfldb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_bench = cvxopt_matrix(np.vstack((-np.eye(X.shape[0]),np.eye(X.shape[0]))))"
      ],
      "metadata": {
        "id": "mQ7b_Hhlfo2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_bench = cvxopt_matrix(np.hstack((np.zeros(X.shape[0]), np.ones(X.shape[0]) * C)))"
      ],
      "metadata": {
        "id": "1wzGmTLufpNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_bench = cvxopt_matrix(y.reshape(1, -1))"
      ],
      "metadata": {
        "id": "G-K3E9rKfpVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = cvxopt_matrix(np.zeros(1))"
      ],
      "metadata": {
        "id": "9xbNwgYEfRmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run solver\n",
        "sol_bench = cvxopt_solvers.qp(P_bench, q_bench, G_bench, h_bench, A_bench, b)\n",
        "alphas_bench = np.array(sol_bench['x'])"
      ],
      "metadata": {
        "id": "Xff0kd_ofTzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "w = ((y * alphas_bench).T @ X).reshape(-1,1)\n",
        "S_bench = ((alphas_bench > 1e-4) & (alphas_bench < C-1e-4)).flatten()\n",
        "b_bench = y[S_bench] - np.sum(Kernel_(X, X[S_bench], type_ = 'default')* y * alphas_bench , axis = 0).reshape(-1,1)\n",
        "\n",
        "print('Alphas = ', alphas_bench[(alphas_bench > 1e-4) & (alphas_bench < C-1e-4)])\n",
        "print('')\n",
        "print('w = ', w.flatten())\n",
        "print('')\n",
        "print('b = ', np.mean(b_bench))\n",
        "print('')\n",
        "print(\"support vector : \", np.array(range(X.shape[0]))[S_bench])"
      ],
      "metadata": {
        "id": "FZRZ05_IVCWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "pred_sol = np.sign(np.sum(Kernel_(X_val, X_val, type_ = 'default')* y_val * alphas_bench , axis = 0).reshape(-1, 1) + np.mean(b_bench))\n",
        "acc_precision_recall(Convolution(pred_sol, y_val))"
      ],
      "metadata": {
        "id": "r1jZHZyhr8Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94788abd-5f7e-45af-a59b-d8de64fb118d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15166666666666667, 0.6650717703349283, 0.0660332541567696)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "#EFSVM 구현(Negative class(Yneg)들에 대한 Entropy\n",
        "########################################\n",
        "\n",
        "\n",
        "# You could use the following function to calculate the entropy\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# You can use the following function to calculate the Euclidean distance (distance.cdist)\n",
        "from scipy.spatial import distance\n",
        "\n",
        "Ypos = y_train[y_train==1].index\n",
        "Yneg = y_train[y_train!=1].index\n",
        "\n",
        "Entropy_Yneg = pd.DataFrame()\n",
        "distNeg = pd.DataFrame(distance.cdist(X_train, X_train.loc[Yneg], 'euclidean'), index=X_train.index, columns=Yneg)"
      ],
      "metadata": {
        "id": "lKE3Fg_fVCZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distNeg"
      ],
      "metadata": {
        "id": "ptpKOglaVj5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "C = 10000\n",
        "Gamma = 2 \n",
        "k = 7 # Number of Nearest Neighbor\n",
        "m = 10\n",
        "beta = 1/18"
      ],
      "metadata": {
        "id": "rUkjCk1qVo8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#EFSVM 구현 (Negative class (Yneg)들에 대한 Entropy)\n",
        "#1. index 별로 Euclidean을 기준으로 가까운 k개 numP, numN, probP, probN, H 저장\n",
        "########################################################\n",
        "\n",
        "for i in Yneg:\n",
        "    numP = np.sum(y_train.loc[distNeg.loc[:,i].sort_values()[1:k+1].index] == 1)\n",
        "    numN = k - numP\n",
        "    probP = numP/k\n",
        "    probN = numN/k\n",
        "    H = entropy([probP, probN])\n",
        "    Entropy_Yneg.loc[i, 'numP'] = numP\n",
        "    Entropy_Yneg.loc[i, 'numN'] = numN\n",
        "    Entropy_Yneg.loc[i, 'probP'] = probP\n",
        "    Entropy_Yneg.loc[i, 'probN'] = probN\n",
        "    Entropy_Yneg.loc[i, 'H'] = H\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "BwSTx8y5Vxy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "#EFSVM 구현 (subset 나누기, FM_l구하기, si 구하기)\n",
        "#1. 구해진 엔트로피를 기준으로 구획을 m만큼 나누기\n",
        "#2. 나뉘어진 subset을 FM_l을 할당 *1-beta*(m-l)\n",
        "#3. Fuzzy membership s_i 설정 (Ypos =1, Yneg=FM_l)\n",
        "########################################################\n",
        "Emax, Emin = Entropy_Yneg['H'].max(), Entropy_Yneg['H'].min()\n",
        "FM = {}\n",
        "for l in range(1, m+1):\n",
        "    thrUp = Emin + l/m * (Emax - Emin)\n",
        "    thrLow = Emin + (l-1)/m * (Emax - Emin)\n",
        "    if m == l:\n",
        "      Entropy_Yneg.loc[(Entropy_Yneg['H'] >= thrLow) &(Entropy_Yneg['H'] <= thrUp), 'subi' ] = l\n",
        "      Entropy_Yneg.loc[(Entropy_Yneg['H'] >= thrLow) &(Entropy_Yneg['H'] <= thrUp), 'FM' ] = 1 - beta * (l-1)\n",
        "    else:\n",
        "      Entropy_Yneg.loc[(Entropy_Yneg['H'] >= thrLow) &(Entropy_Yneg['H'] < thrUp), 'subi' ] = l\n",
        "      Entropy_Yneg.loc[(Entropy_Yneg['H'] >= thrLow) &(Entropy_Yneg['H'] < thrUp), 'FM' ] = 1 - beta * (l-1)\n",
        "\n",
        "si = pd.DataFrame(index=X_train.index)\n",
        "si.loc[Ypos, 'si']=1\n",
        "si.loc[Entropy_Yneg.index, 'si'] = Entropy_Yneg['FM'].values\n",
        "si = np.array(si['si'])"
      ],
      "metadata": {
        "id": "xdXBsWuBV7e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kernel 관련 정의 numpy array 형식으로 생성\n",
        "\n",
        "H_ef = Kernel_(X, X, params = Gamma, type_ = 'rbf') * 1.\n",
        "H_ef *= y@y.T \n",
        "P_ef = cvxopt_matrix(H_ef)\n",
        "q_ef = cvxopt_matrix(-np.ones((X.shape[0], 1)))\n",
        "G_ef = cvxopt_matrix(np.vstack((-np.eye(X.shape[0]), np.eye(X.shape[0]))))\n",
        "h_ef = cvxopt_matrix(np.hstack((np.zeros(X.shape[0]), si * C)))\n",
        "A_ef = cvxopt_matrix(y.reshape(1, -1))\n",
        "b = cvxopt_matrix(np.zeros(1))\n",
        "\n",
        "#Run solver\n",
        "sol_ef = cvxopt_solvers.qp(P_ef, q_ef, G_ef, h_ef, A_ef, b)\n",
        "alphas_ef = np.array(sol_ef['x'])\n",
        "\n",
        "#Results\n",
        "S_ef = ((alphas_ef > 1e-4) & (alphas_ef < C-1e-4)).flatten()\n",
        "b_ef = y[S_ef] - np.sum(Kernel_(X, X[S_ef]  , params = 2, type_ = 'rbf')* y * alphas_ef , axis = 0).reshape(-1,1)\n",
        "print('Alphas = ',alphas_ef[(alphas_ef > 1e-4) & (alphas_ef < C-1e-4)])\n",
        "print('')\n",
        "print('b = ', b_ef[0])\n",
        "print('')\n",
        "print(\"support vector : \", np.array(range(X_shape[0]))[S_ef])"
      ],
      "metadata": {
        "id": "eEZgJULDDA38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sol = np.sign(np.sum(Kernel_(X_val, X_val, params = 2, type_ = 'rbf') * y_val * alphas_ef , axis = 0).reshape(-1,1) + b_ef[0])\n",
        "acc_precision_recall(Convolution(pred_sol,y_val))"
      ],
      "metadata": {
        "id": "WWW9YRsZDGyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1165c9ec-7970-4ce3-dd8e-5c457510a3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.415, 0.8230414746543778, 0.42422802850356295)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeLpAeWVo1rW"
      },
      "source": [
        "## 3. System Implementation\n",
        "- df_test에 기반하여 실제 사용을 위한 system implemetation 작업 진행\n",
        "- Performance에 대한 평가데이터는 df_test로 지난 프로젝트와 마찬가지로 Data Pre-processing이 System implementation에 동시 구현\n",
        "- 특히, scaler를 사용할 경우 cache를 반드시 사용하여 올바른 system implementation이 되도록 할 것!!! (주의!!!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Y3dWkknnB2"
      },
      "source": [
        "df1_test=df_test.dropna(axis=0)\n",
        "\n",
        "df1_test = df1_test.replace({'sub_grade' :'A1'},1)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'A2'},2)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'A3'},3)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'A4'},4)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'A5'},5)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'B1'},6)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'B2'},7)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'B3'},8)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'B4'},9)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'B5'},10)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'C1'},11)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'C2'},12)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'C3'},13)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'C4'},14)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'C5'},15)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'D1'},16)\n",
        "df1_test = df1_test.replace({'sub_grade' :'D2'},17)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'D3'},18)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'D4'},19)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'D5'},20)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'E1'},21)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'E2'},22)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'E3'},23)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'E4'},24)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'E5'},25)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'F1'},26)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'F2'},27)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'F3'},28)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'F4'},29)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'F5'},30)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'G1'},31)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'G2'},32)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'G3'},33)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'G4'},34)\n",
        "df1_test = df1_test.replace({'sub_grade' : 'G5'},35)\n",
        "\n",
        "df1_test = df1_test.drop('grade', axis=1)\n",
        "\n",
        "df1_test = df1_test.replace({'emp_length' :'10+ years'},10)\n",
        "df1_test = df1_test.replace({'emp_length' :'9 years'},9)\n",
        "df1_test = df1_test.replace({'emp_length' :'8 years'},8)\n",
        "df1_test = df1_test.replace({'emp_length' :'7 years'},7)\n",
        "df1_test = df1_test.replace({'emp_length' :'6 years'},6)\n",
        "df1_test = df1_test.replace({'emp_length' :'5 years'},5)\n",
        "df1_test = df1_test.replace({'emp_length' :'4 years'},4)\n",
        "df1_test = df1_test.replace({'emp_length' :'3 years'},3)\n",
        "df1_test = df1_test.replace({'emp_length' :'2 years'},2)\n",
        "df1_test = df1_test.replace({'emp_length' :'1 year'},1)\n",
        "df1_test = df1_test.replace({'emp_length' :'< 1 year'},0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_test = df1_test.copy()\n",
        "\n",
        "for col in to_dummy:\n",
        "  ratios = ratio_dicts[col]\n",
        "  for index, ele in enumerate(ratios):\n",
        "    df2_test = df2_test.replace(ele[0], index) # ele[0] : 원소 이름, index : ratio가 오름차순으로 정렬되어서, 0이 가장 ratio가 가장 낮음"
      ],
      "metadata": {
        "id": "hxv8nyPfl6fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_test = df2_test.replace({'loan_status' :'Charged Off'},0)\n",
        "df2_test = df2_test.replace({'loan_status' :'Fully Paid'},1)\n",
        "df2_test = df2_test.replace({'initial_list_status':'w'},0)\n",
        "df2_test = df2_test.replace({'initial_list_status':'f'},1)"
      ],
      "metadata": {
        "id": "rL9Oq9y5mAVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = Standar_val(np.array(df2_test.drop(columns=['loan_status'])), avg=cache_avg, std=cache_std)\n",
        "X_test = np.nan_to_num(X_test)\n",
        "\n",
        "y_test = np.array(df2_test['loan_status'])* 1.\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "t82czXnWmCOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "pred_sol = np.sign(np.sum(Kernel_(X_test, X_test, type_ = 'default') * y_test * alphas, axis=0).reshape(-1,1) + b_lin[0])\n",
        "acc_precision_recall(Convolution(pred_sol, y_test))"
      ],
      "metadata": {
        "id": "nOc0zSW4mE1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2da4f8c-84e9-4070-9fa7-014db8b17d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.740308019118428, 0.9824120603015075, 0.7148080438756855)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "\n",
        "pred_sol = np.sign(np.sum(Kernel_(X_test, X_test, params = 2, type_ = 'rbf') * y_test * alphas , axis = 0).reshape(-1,1) + b[0])\n",
        "acc_precision_recall(Convolution(pred_sol, y_test))"
      ],
      "metadata": {
        "id": "68GeWd4NnGAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00ae64e-07b8-4dc1-a296-fed738a61073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8714816781731279, 0.8714816781731279, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}